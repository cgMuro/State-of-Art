Generative Pre-trained Transformer (GPT) is an autoregressive language model by OpenAI. Until now, there have been three versions of GPT.

The model's architecture is a stack of multiple decoder-only transformers, and it's trained on a larger corpus of text from the internet without any supervision (i.e. unsupervised pre-training).
GPT learns the statistical relationships between words by performing the simple task of predicting the *next word* in a sentence, given the previous words.

GPT can also be fine-tuned on specific tasks by adding a final output linear layer and then briefly trained again.

<br>

## Resources:
* Paper: [*Language Models are Few-Shot Learners*](https://arxiv.org/abs/2005.14165) (**Tom B. Brown et al.,  2020**)
* [*Unicorn AI - Computerphile*](https://www.youtube.com/watch?v=89A4jGvaaKk) (**Computerphile**)
* [*More GPT-2, the 'writer' of Unicorn AI - Computerphile*](https://www.youtube.com/watch?v=p-6F4rhRYLQ) (**Computerphile**)
* [*GPT-2: Why Didn't They Release It? - Computerphile*](https://www.youtube.com/watch?v=AJxLtdur5fc) (**Computerphile**)
* [*GPT3: An Even Bigger Language Model - Computerphile*](https://www.youtube.com/watch?v=_8yVOC4ciXc) (**Computerphile**)
* [*GPT-3: Language Models are Few-Shot Learners (Paper Explained)*](https://www.youtube.com/watch?v=SY5PvZrJhLE) (**Yannic Kilcher**
* [*karpathy/minGPT*](https://github.com/karpathy/minGPT) (**Andrej karpathy**)
* [The Illustrated GPT-2 (Visualizing Transformer Language Models](https://jalammar.github.io/illustrated-gpt2/) (**Jay Alammar**)
* [How GPT3 Works - Visualizations and Animations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/) (**Jay Alammar**)
* [How GPT3 Works - Easily Explained with Animations](https://www.youtube.com/watch?v=MQnJZuBGmSQ) (**Jay Alammar**)
