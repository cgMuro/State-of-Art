This repository contains an excursion into the history and the future of AI and deep learning.

Starting from Denny Britz's article [*Deep Learning's Most Important Ideas - A Brief Historical Review*](https://dennybritz.com/blog/deep-learning-most-important-ideas/), I used the resources proposed and more to code and learn about some of the *most important ideas in deep learning*.

Currently, I'm expanding from the suggestions of the article linked above and exploring even more techniques and architectures that are part of the *State of Art* in AI or that I find particularly interesting.

I linked all the resources used for both the theoretical understanding and the code implementations. And I will try to build the models using as many libraries (PyTorch, TensorFlow) and programming languages (Python, C++) as I can.

<br>

#### Topics covered
* A2C
* A3C
* ACKTR
* Adam Optimizer
* AlexNet
* Bidirectional Encoder Representations from Transformers (BERT)
* Contrastive Language–Image Pre-training (CLIP)
* DALL•E
* Deep Q-Learning
* Sequence to Sequence Network with Attention
* Generative Adversarial Network (GAN)
* Generative Pre-trained Transformer (GPT)
* ResNet
* Transformer (both in Computer Vision and Natural Language Processing)
* WaveNet

#### Next topics on the list
* MuZero (DeepMind)
* TransGANs
* FermiNet (DeepMind)
